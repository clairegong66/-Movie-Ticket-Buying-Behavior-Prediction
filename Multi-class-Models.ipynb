{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/gloriatang/Downloads/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('modeling_data_v2.csv')\n",
    "oot=pd.read_csv('oot_data_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HASHED_ATOM_CUSTOMER_ID', 'OS_TYPE_e', 'PRODUCTION_ID', 'RATINGS',\n",
       "       'MOVIE_DETAILS_PAGES_VIEWED_BY_CUSTOMER',\n",
       "       'MOVIE_DETAILS_PAGES_VIEWED_BY_ALL_CUSTOMERS',\n",
       "       'USER_SHARE_OF_ALL_MOVIE_DETAIL_PAGES', 'WANT_TO_SEE_CLICKS',\n",
       "       'TRAILER_VIEWS', 'TICKETS',\n",
       "       ...\n",
       "       'count_HASHED_ATOM_CUSTOMER_ID_1d_ratings',\n",
       "       'count_HASHED_ATOM_CUSTOMER_ID_3d_ratings',\n",
       "       'count_HASHED_ATOM_CUSTOMER_ID_7d_ratings',\n",
       "       'count_HASHED_ATOM_CUSTOMER_ID_14d_ratings',\n",
       "       'count_HASHED_ATOM_CUSTOMER_ID_28d_ratings',\n",
       "       'count_HASHED_ATOM_CUSTOMER_ID_30d_ratings',\n",
       "       'count_HASHED_ATOM_CUSTOMER_ID_35d_ratings',\n",
       "       'count_HASHED_ATOM_CUSTOMER_ID_42d_ratings',\n",
       "       'count_HASHED_ATOM_CUSTOMER_ID_49d_ratings',\n",
       "       'count_HASHED_ATOM_CUSTOMER_ID_56d_ratings'],\n",
       "      dtype='object', length=119)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns\n",
    "#data1=data1.drop(data1.columns[0:3],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781036, 119)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with categorical variables\n",
    "## Drop useless columns\n",
    "\n",
    "columns = ['TICKETS_CLASS','PRODUCTION_ID','RELEASE_DATE','DATE','HASHED_ATOM_CUSTOMER_ID','TICKETS']\n",
    "y=data['TICKETS_CLASS']\n",
    "X = data.drop(columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1781036, 113) (1781036,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-hot encoding for 'WEEKDAY_OF_ENGAGEMENT','OS_TYPE_e'\n",
    "X_dummy=pd.get_dummies(X, prefix_sep='_',columns=['WEEKDAY_OF_ENGAGEMENT','OS_TYPE_e'])\n",
    "## Convert DATE_TO_RELEASE to numeric\n",
    "X_dummy[\"DATE_TO_RELEASE\"]=X_dummy[\"DATE_TO_RELEASE\"].str.replace('+','')\n",
    "X_dummy[\"DATE_TO_RELEASE\"]=X_dummy[\"DATE_TO_RELEASE\"].str.replace(' days 00:00:00.000000000','').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1246725, 121) (1246725,)\n",
      "(534311, 121) (534311,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(666)\n",
    "X_train, X_test, y_train,y_test = train_test_split(X_dummy, y, test_size = 0.3)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NB = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#negative X not allowed\n",
    "X_train_NB = X_train.drop(['DATE_TO_RELEASE'], axis=1)\n",
    "X_test_NB = X_test.drop(['DATE_TO_RELEASE'], axis=1)\n",
    "NB.fit(X_train_NB, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value  Group Tickets  None  One Ticket  Two Tickets\n",
      "Actual Value                                                 \n",
      "Group Tickets              411   224         843        11085\n",
      "None                     14536  7468       28996       389128\n",
      "One Ticket                1665   876        3299        45706\n",
      "Two Tickets               1031   524        1928        26591\n",
      "Accurancy: 6.760669348\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.73      0.02      0.03    440791\n",
      "        1.0       0.13      0.09      0.11     51273\n",
      "        2.0       0.05      0.78      0.09     29620\n",
      "        3.0       0.11      0.15      0.13     12627\n",
      "\n",
      "avg / total       0.62      0.07      0.04    534311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicting the test set results\n",
    "y_pred_NB = NB.predict(X_test_NB)\n",
    "\n",
    "y_pred_NB_name = np.vectorize(reversefactor.get)(y_pred_NB)\n",
    "# Making the Confusion Matrix & show result\n",
    "print(pd.crosstab(y_test_name, y_pred_NB_name, rownames=['Actual Value'], colnames=['Predicted Value']))\n",
    "print('Accurancy:',NB.score(X_test_NB, y_test)*100 )\n",
    "\n",
    "print(classification_report(y_test, y_pred_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1: 0.10585204418243344\n"
     ]
    }
   ],
   "source": [
    "Weighted_F1 = (0.11*51273+0.09*29620+0.13*12627)/(51273+30074+12627)\n",
    "print('Weighted F1:',Weighted_F1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.01      0.03   1026842\n",
      "        1.0       0.13      0.09      0.11    120166\n",
      "        2.0       0.05      0.77      0.09     70102\n",
      "        3.0       0.11      0.15      0.12     29615\n",
      "\n",
      "avg / total       0.61      0.07      0.04   1246725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#trian\n",
    "y_train_NB = NB.predict(X_train_NB)\n",
    "print(classification_report(y_train, y_train_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1: 0.1049705525211135\n"
     ]
    }
   ],
   "source": [
    "Weighted_train_NB = (0.11*120166+0.09*70102+0.12*29615)/(120166+70102+29615)\n",
    "print('Weighted F1:',Weighted_train_NB )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scalingï¼Œ good for logistic regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_l = scaler.fit_transform(X_train)\n",
    "X_test_l = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train_l, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value  Group Tickets    None  One Ticket  Two Tickets\n",
      "Actual Value                                                   \n",
      "Group Tickets               29    9929        2504          165\n",
      "None                        37  436969        3513          272\n",
      "One Ticket                  52   39100       11807          314\n",
      "Two Tickets                 19   23194        6007          400\n",
      "Accurancy: 84.0718233388\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.86      0.99      0.92    440791\n",
      "        1.0       0.50      0.23      0.31     51273\n",
      "        2.0       0.35      0.01      0.03     29620\n",
      "        3.0       0.21      0.00      0.00     12627\n",
      "\n",
      "avg / total       0.78      0.84      0.79    534311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred_log = logreg.predict(X_test_l)\n",
    "#Reverse factorize (converting y_pred from 0,1,2 and 3)\n",
    "definitions = ['None','One Ticket','Two Tickets','Group Tickets']\n",
    "reversefactor = dict(zip(range(4),definitions))\n",
    "y_test_name = np.vectorize(reversefactor.get)(y_test)\n",
    "y_pred_log_name = np.vectorize(reversefactor.get)(y_pred_log)\n",
    "# Making the Confusion Matrix\n",
    "print(pd.crosstab(y_test_name, y_pred_log_name, rownames=['Actual Value'], colnames=['Predicted Value']))\n",
    "print('Accurancy:',logreg.score(X_test_l, y_test)*100 )\n",
    "\n",
    "# Evaluate model accuracy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1: 0.1794613986313088\n"
     ]
    }
   ],
   "source": [
    "Weighted_F1_lg = (0.31*51273+0.03*29620+0.0*12627)/(51273+29620+12627)\n",
    "print('Weighted F1:',Weighted_F1_lg )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.86      0.99      0.92   1026842\n",
      "        1.0       0.49      0.23      0.31    120166\n",
      "        2.0       0.39      0.01      0.03     70102\n",
      "        3.0       0.24      0.00      0.00     29615\n",
      "\n",
      "avg / total       0.78      0.84      0.79   1246725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##train\n",
    "y_train_log = logreg.predict(X_train_l)\n",
    "print(classification_report(y_train, y_train_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1: 0.1789793662993501\n"
     ]
    }
   ],
   "source": [
    "Weighted_train_lg = (0.31*120166+0.03*70102+0.0*29615)/(120166+70102+29615)\n",
    "print('Weighted F1:',Weighted_train_lg )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   12.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 3 of 11\n",
      "building tree 4 of 11\n",
      "building tree 5 of 11\n",
      "building tree 6 of 11\n",
      "building tree 7 of 11\n",
      "building tree 8 of 11\n",
      "building tree 9 of 11\n",
      "building tree 10 of 11\n",
      "building tree 11 of 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=11, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=3, warm_start=False)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 11, criterion = 'entropy', random_state = 0,verbose = 3)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value  Group Tickets    None  One Ticket  Two Tickets\n",
      "Actual Value                                                   \n",
      "Group Tickets              960    4981        4672         2014\n",
      "None                      2017  420876       11705         6193\n",
      "One Ticket                2086   18607       23786         6794\n",
      "Two Tickets               1193   13593       10160         4674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    2.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurancy: 84.2760115364\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.92      0.95      0.94    440791\n",
      "        1.0       0.47      0.46      0.47     51273\n",
      "        2.0       0.24      0.16      0.19     29620\n",
      "        3.0       0.15      0.08      0.10     12627\n",
      "\n",
      "avg / total       0.82      0.84      0.83    534311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicting the test set results\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "y_pred_rf_name = np.vectorize(reversefactor.get)(y_pred_rf)\n",
    "# Making the Confusion Matrix & show result\n",
    "print(pd.crosstab(y_test_name, y_pred_rf_name, rownames=['Actual Value'], colnames=['Predicted Value']))\n",
    "print('Accurancy:',rf.score(X_test, y_test)*100 )\n",
    "\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1: 0.331360243798118\n"
     ]
    }
   ],
   "source": [
    "Weighted_F1_rf = (0.47*51273+0.19*29620+0.1*12627)/(51273+29620+12627)\n",
    "print('Weighted F1:',Weighted_F1_rf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    5.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      0.99      0.99   1026842\n",
      "        1.0       0.93      0.89      0.91    120166\n",
      "        2.0       0.95      0.85      0.89     70102\n",
      "        3.0       0.96      0.85      0.90     29615\n",
      "\n",
      "avg / total       0.97      0.97      0.97   1246725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "y_train_rf = rf.predict(X_train)\n",
    "print(classification_report(y_train, y_train_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1: 0.9022768472323918\n"
     ]
    }
   ],
   "source": [
    "Weighted_train_rf = (0.91*120166+0.89*70102+0.90*29615)/(120166+70102+29615)\n",
    "print('Weighted F1:',Weighted_train_rf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xg_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.140947\ttest-merror:0.139707\n",
      "[1]\ttrain-merror:0.137724\ttest-merror:0.136572\n",
      "[2]\ttrain-merror:0.137233\ttest-merror:0.136175\n",
      "[3]\ttrain-merror:0.136323\ttest-merror:0.135333\n",
      "[4]\ttrain-merror:0.135756\ttest-merror:0.134865\n"
     ]
    }
   ],
   "source": [
    "#setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['silent'] = 1\n",
    "param['num_class'] = 4\n",
    "\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 5\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# get prediction\n",
    "y_pred_xgb = bst.predict(xg_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value  Group Tickets    None  One Ticket  Two Tickets\n",
      "Actual Value                                                   \n",
      "Group Tickets               26    6984        5097          520\n",
      "None                         1  437933        2609          248\n",
      "One Ticket                   1   27694       22708          870\n",
      "Two Tickets                  6   18179        9851         1584\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      0.99      0.94    440791\n",
      "        1.0       0.56      0.44      0.50     51273\n",
      "        2.0       0.49      0.05      0.10     29620\n",
      "        3.0       0.76      0.00      0.00     12627\n",
      "\n",
      "avg / total       0.84      0.87      0.83    534311\n",
      "\n",
      "Accurancy: 0.865134724907\n"
     ]
    }
   ],
   "source": [
    "y_pred_xgb_name = np.vectorize(reversefactor.get)(y_pred_xgb)\n",
    "# Making the Confusion Matrix & show result\n",
    "print(pd.crosstab(y_test_name, y_pred_xgb_name, rownames=['Actual Value'], colnames=['Predicted Value']))\n",
    "\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "acc = np.sum(y_pred_xgb == y_test) / y_test.shape[0]\n",
    "print('Accurancy:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1: 0.3058008982035928\n"
     ]
    }
   ],
   "source": [
    "Weighted_F1_xg = (0.5*51273+0.1*29620+0.0*12627)/(51273+29620+12627)\n",
    "print('Weighted F1:',Weighted_F1_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      0.99      0.94   1026842\n",
      "        1.0       0.56      0.44      0.49    120166\n",
      "        2.0       0.52      0.06      0.10     70102\n",
      "        3.0       0.78      0.00      0.00     29615\n",
      "\n",
      "avg / total       0.84      0.86      0.83   1246725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_xgb = bst.predict(xg_train)\n",
    "print(classification_report(y_train, y_train_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1: 0.299666368022994\n"
     ]
    }
   ],
   "source": [
    "Weighted_train_xgb = (0.49*120166+0.10*70102+0.0*29615)/(120166+70102+29615)\n",
    "print('Weighted F1:',Weighted_train_xgb )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance in OOT data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Follow the same process\n",
    "\n",
    "# Deal with categorical variables\n",
    "## Drop useless columns\n",
    "columns = ['TICKETS_CLASS','PRODUCTION_ID','RELEASE_DATE','DATE','HASHED_ATOM_CUSTOMER_ID','TICKETS']\n",
    "oot_y=oot['TICKETS_CLASS']\n",
    "oot_X = oot.drop(columns, axis=1)\n",
    "\n",
    "## One-hot encoding for 'WEEKDAY_OF_ENGAGEMENT','OS_TYPE_e'\n",
    "oot_X=pd.get_dummies(oot_X, prefix_sep='_',columns=['WEEKDAY_OF_ENGAGEMENT','OS_TYPE_e'])\n",
    "## Convert DATE_TO_RELEASE to numeric\n",
    "oot_X[\"DATE_TO_RELEASE\"]=oot_X[\"DATE_TO_RELEASE\"].str.replace('+','')\n",
    "oot_X[\"DATE_TO_RELEASE\"]=oot_X[\"DATE_TO_RELEASE\"].str.replace(' days 00:00:00.000000000','').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.70      0.82      0.76    327793\n",
      "        1.0       0.13      0.01      0.01     79457\n",
      "        2.0       0.02      0.03      0.03     29107\n",
      "        3.0       0.00      0.01      0.01      8682\n",
      "\n",
      "avg / total       0.54      0.61      0.56    445039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NB\n",
    "X_oot_NB = oot_X.drop(['DATE_TO_RELEASE'], axis=1)\n",
    "\n",
    "y_oot_NB = NB.predict(X_oot_NB)\n",
    "print(classification_report(oot_y, y_oot_NB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1: 0.014965116080719171\n"
     ]
    }
   ],
   "source": [
    "Weighted_F1_oot_NB = (0.01*79457+0.03*29107+0.01*8682)/(79457+29107+8682)\n",
    "print('Weighted F1:',Weighted_F1_oot_NB )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "oot_X = scaler.fit_transform(oot_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.75      0.98      0.85    327793\n",
      "        1.0       0.12      0.01      0.02     79457\n",
      "        2.0       0.22      0.07      0.10     29107\n",
      "        3.0       0.05      0.02      0.03      8682\n",
      "\n",
      "avg / total       0.59      0.73      0.64    445039\n",
      "\n",
      "Accurancy: 72.9488876256\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "y_oot_log = logreg.predict(oot_X)\n",
    "print(classification_report(oot_y, y_oot_log))\n",
    "print('Accurancy:',logreg.score(oot_X, oot_y)*100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1: 0.040600958668099554\n"
     ]
    }
   ],
   "source": [
    "Weighted_F1_oot_log = (0.02*79457+0.10*29107+0.03*8682)/(79457+29107+8682)\n",
    "print('Weighted F1:',Weighted_F1_oot_log )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.79      0.91      0.85    327793\n",
      "        1.0       0.43      0.22      0.29     79457\n",
      "        2.0       0.10      0.08      0.09     29107\n",
      "        3.0       0.03      0.00      0.00      8682\n",
      "\n",
      "avg / total       0.66      0.72      0.68    445039\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurancy: 71.8164026074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "\n",
    "y_oot_rf = rf.predict(oot_X)\n",
    "print(classification_report(oot_y, y_oot_rf))\n",
    "print('Accurancy:',rf.score(oot_X, oot_y)*100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1: 0.21887450318134521\n"
     ]
    }
   ],
   "source": [
    "Weighted_F1_oot_rf = (0.29*79457+0.09*29107+0.00*8682)/(79457+29107+8682)\n",
    "print('Weighted F1:',Weighted_F1_oot_rf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.75      0.94      0.84    327793\n",
      "        1.0       0.46      0.15      0.23     79457\n",
      "        2.0       0.01      0.00      0.01     29107\n",
      "        3.0       0.00      0.00      0.00      8682\n",
      "\n",
      "avg / total       0.64      0.72      0.66    445039\n",
      "\n",
      "Accurancy: 0.716633823103\n"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "xg_oot = xgb.DMatrix(oot_X, label=oot_y)\n",
    "y_oot_xgb = bst.predict(xg_oot)\n",
    "print(classification_report(oot_y, y_oot_xgb))\n",
    "acc = np.sum(y_oot_xgb == oot_y) / oot_y.shape[0]\n",
    "print('Accurancy:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1: 0.15835235317196322\n"
     ]
    }
   ],
   "source": [
    "Weighted_F1_oot_xg = (0.23*79457+0.01*29107+0.00*8682)/(79457+29107+8682)\n",
    "print('Weighted F1:',Weighted_F1_oot_xg )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step: Tune best model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Grid Search for parameters\n",
    "1. **N_estimators**: n_estimators represents the number of trees in the forest. Usually the higher the number of trees the better to learn the data. However, adding a lot of trees can slow down the training process considerably\n",
    "2. **max_depth**: max_depth represents the depth of each tree in the forest. The deeper the tree, the more splits it has and it captures more information about the data. \n",
    "3. **min_samples_split**: min_samples_split represents the minimum number of samples required to split an internal node. This can vary between considering at least one sample at each node to considering all of the samples at each node. When we increase this parameter, each tree in the forest becomes more constrained as it has to consider more samples at each node.\n",
    "4. **min_samples_leaf**: min_samples_leaf is The minimum number of samples required to be at a leaf node. This parameter is similar to min_samples_splits, however, this describe the minimum number of samples of samples at the leafs, the base of the tree.\n",
    "5. **max_features**: max_features represents the number of features to consider when looking for the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1246725, 121) (1246725,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [10, 50, 90, 130, 170, 210], 'max_features': [0.20000000000000001, 0.40000000000000002, 0.59999999999999998, 0.80000000000000004, 1.0, 'sqrt'], 'max_depth': [5, 16, 27, 38, 50, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 210, num = 6)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = [round(x, 2) for x in np.linspace(start = 0.2, stop = 1, num = 5)]\n",
    "max_features.append('sqrt')\n",
    "## Empirical good default values are max_features=n_features for regression problems, \n",
    "## and max_features = sqrt(n_features) for classification tasks\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 50, num = 5)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "[CV] n_estimators=90, min_samples_split=2, min_samples_leaf=4, max_features=0.2, max_depth=27, bootstrap=False \n",
      "[CV] n_estimators=90, min_samples_split=2, min_samples_leaf=4, max_features=0.2, max_depth=27, bootstrap=False \n",
      "[CV] n_estimators=90, min_samples_split=2, min_samples_leaf=4, max_features=0.2, max_depth=27, bootstrap=False \n",
      "[CV] n_estimators=90, min_samples_split=5, min_samples_leaf=2, max_features=0.2, max_depth=38, bootstrap=True \n",
      "[CV]  n_estimators=90, min_samples_split=5, min_samples_leaf=2, max_features=0.2, max_depth=38, bootstrap=True, total=25.9min\n",
      "[CV] n_estimators=90, min_samples_split=5, min_samples_leaf=2, max_features=0.2, max_depth=38, bootstrap=True \n",
      "[CV]  n_estimators=90, min_samples_split=2, min_samples_leaf=4, max_features=0.2, max_depth=27, bootstrap=False, total=32.8min\n",
      "[CV] n_estimators=90, min_samples_split=5, min_samples_leaf=2, max_features=0.2, max_depth=38, bootstrap=True \n",
      "[CV]  n_estimators=90, min_samples_split=2, min_samples_leaf=4, max_features=0.2, max_depth=27, bootstrap=False, total=32.8min\n",
      "[CV] n_estimators=50, min_samples_split=2, min_samples_leaf=2, max_features=0.6, max_depth=16, bootstrap=False \n",
      "[CV]  n_estimators=90, min_samples_split=2, min_samples_leaf=4, max_features=0.2, max_depth=27, bootstrap=False, total=33.1min\n",
      "[CV] n_estimators=50, min_samples_split=2, min_samples_leaf=2, max_features=0.6, max_depth=16, bootstrap=False \n",
      "[CV]  n_estimators=90, min_samples_split=5, min_samples_leaf=2, max_features=0.2, max_depth=38, bootstrap=True, total=27.3min\n",
      "[CV] n_estimators=50, min_samples_split=2, min_samples_leaf=2, max_features=0.6, max_depth=16, bootstrap=False \n",
      "[CV]  n_estimators=90, min_samples_split=5, min_samples_leaf=2, max_features=0.2, max_depth=38, bootstrap=True, total=28.7min\n",
      "[CV] n_estimators=90, min_samples_split=2, min_samples_leaf=4, max_features=0.2, max_depth=None, bootstrap=False \n",
      "[CV]  n_estimators=50, min_samples_split=2, min_samples_leaf=2, max_features=0.6, max_depth=16, bootstrap=False, total=45.4min\n",
      "[CV]  n_estimators=50, min_samples_split=2, min_samples_leaf=2, max_features=0.6, max_depth=16, bootstrap=False, total=45.6min\n",
      "[CV] n_estimators=90, min_samples_split=2, min_samples_leaf=4, max_features=0.2, max_depth=None, bootstrap=False \n",
      "[CV] n_estimators=90, min_samples_split=2, min_samples_leaf=4, max_features=0.2, max_depth=None, bootstrap=False \n",
      "[CV]  n_estimators=90, min_samples_split=2, min_samples_leaf=4, max_features=0.2, max_depth=None, bootstrap=False, total=29.2min\n",
      "[CV] n_estimators=130, min_samples_split=2, min_samples_leaf=1, max_features=0.8, max_depth=50, bootstrap=True \n",
      "[CV]  n_estimators=50, min_samples_split=2, min_samples_leaf=2, max_features=0.6, max_depth=16, bootstrap=False, total=41.4min\n",
      "[CV] n_estimators=130, min_samples_split=2, min_samples_leaf=1, max_features=0.8, max_depth=50, bootstrap=True \n",
      "[CV]  n_estimators=90, min_samples_split=2, min_samples_leaf=4, max_features=0.2, max_depth=None, bootstrap=False, total=28.9min\n",
      "[CV]  n_estimators=90, min_samples_split=2, min_samples_leaf=4, max_features=0.2, max_depth=None, bootstrap=False, total=28.9min\n",
      "[CV] n_estimators=130, min_samples_split=2, min_samples_leaf=1, max_features=0.8, max_depth=50, bootstrap=True \n",
      "[CV] n_estimators=90, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=16, bootstrap=False \n",
      "[CV]  n_estimators=90, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=16, bootstrap=False, total=13.1min\n",
      "[CV] n_estimators=90, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=16, bootstrap=False \n",
      "[CV]  n_estimators=90, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=16, bootstrap=False, total=12.7min\n",
      "[CV] n_estimators=90, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=16, bootstrap=False \n",
      "[CV]  n_estimators=90, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=16, bootstrap=False, total=13.2min\n",
      "[CV] n_estimators=170, min_samples_split=5, min_samples_leaf=4, max_features=1.0, max_depth=38, bootstrap=True \n",
      "[CV]  n_estimators=130, min_samples_split=2, min_samples_leaf=1, max_features=0.8, max_depth=50, bootstrap=True, total=122.6min\n",
      "[CV] n_estimators=170, min_samples_split=5, min_samples_leaf=4, max_features=1.0, max_depth=38, bootstrap=True \n",
      "[CV]  n_estimators=130, min_samples_split=2, min_samples_leaf=1, max_features=0.8, max_depth=50, bootstrap=True, total=122.0min\n",
      "[CV] n_estimators=170, min_samples_split=5, min_samples_leaf=4, max_features=1.0, max_depth=38, bootstrap=True \n",
      "[CV]  n_estimators=130, min_samples_split=2, min_samples_leaf=1, max_features=0.8, max_depth=50, bootstrap=True, total=124.3min\n",
      "[CV] n_estimators=130, min_samples_split=2, min_samples_leaf=1, max_features=0.2, max_depth=27, bootstrap=False \n"
     ]
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "RF = RandomForestClassifier()\n",
    "\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = RF, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 100, \n",
    "                               cv = 3, \n",
    "                               verbose = 2,\n",
    "                               scoring = 'f1_weighted',\n",
    "                               n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
